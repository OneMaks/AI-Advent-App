# LM Studio Terminal Chat Configuration
# =====================================

# Server Settings
# ---------------
# URL of the LM Studio server (default: http://localhost:1234/v1)
server.url=http://localhost:1234/v1

# Model to use (leave empty to auto-detect first available model)
#model=llama-3.2-1b

# Generation Parameters
# ---------------------
# Controls randomness (0.0 = deterministic, 2.0 = very random)
temperature=0.7

# Maximum number of tokens to generate
max_tokens=2000

# Top-p (nucleus) sampling parameter (0.0-1.0)
top_p=1.0

# Frequency penalty (-2.0 to 2.0)
frequency_penalty=0.0

# Presence penalty (-2.0 to 2.0)
presence_penalty=0.0

# Application Settings
# --------------------
# Enable streaming responses (true/false)
streaming=true

# Enable colored terminal output (true/false)
colors=true

# Maximum context window size (in tokens)
context_window=4096

# Directory for saving conversations
conversations_dir=conversations

# Default system prompt (optional)
#system_prompt=You are a helpful AI assistant.
